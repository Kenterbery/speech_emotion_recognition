{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenterbery/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Ravdess = \"/home/kenterbery/projects/speech_emotion_recognition/input/Ravdess/audio_speech_actors_01-24/\"\n",
    "Crema = \"/home/kenterbery/projects/speech_emotion_recognition/input/Crema/\"\n",
    "Savee = \"/home/kenterbery/projects/speech_emotion_recognition/input/Savee/\"\n",
    "Tess = \"/home/kenterbery/projects/speech_emotion_recognition/input/Tess/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ravdess dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "emotion_df = []\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for wav in actor:\n",
    "        info = wav.partition(\".wav\")[0].split(\"-\")\n",
    "        emotion = int(info[2])\n",
    "        emotion_df.append((emotion, Ravdess + dir + \"/\" + wav))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Ravdess_df = pd.DataFrame.from_dict(emotion_df)\n",
    "Ravdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Path\n0  neutral  /home/kenterbery/projects/speech_emotion_recog...\n1  neutral  /home/kenterbery/projects/speech_emotion_recog...\n2  disgust  /home/kenterbery/projects/speech_emotion_recog...\n3  disgust  /home/kenterbery/projects/speech_emotion_recog...\n4  disgust  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Crema dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Path\n0  disgust  /home/kenterbery/projects/speech_emotion_recog...\n1    happy  /home/kenterbery/projects/speech_emotion_recog...\n2      sad  /home/kenterbery/projects/speech_emotion_recog...\n3    happy  /home/kenterbery/projects/speech_emotion_recog...\n4  neutral  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happy</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = []\n",
    "\n",
    "for wav in os.listdir(Crema):\n",
    "    info = wav.partition(\".wav\")[0].split(\"_\")\n",
    "    if info[2] == 'SAD':\n",
    "        emotion_df.append((\"sad\", Crema + wav))\n",
    "    elif info[2] == 'ANG':\n",
    "        emotion_df.append((\"angry\", Crema + wav))\n",
    "    elif info[2] == 'DIS':\n",
    "        emotion_df.append((\"disgust\", Crema + wav))\n",
    "    elif info[2] == 'FEA':\n",
    "        emotion_df.append((\"fear\", Crema + wav))\n",
    "    elif info[2] == 'HAP':\n",
    "        emotion_df.append((\"happy\", Crema + wav))\n",
    "    elif info[2] == 'NEU':\n",
    "        emotion_df.append((\"neutral\", Crema + wav))\n",
    "    else:\n",
    "        emotion_df.append((\"unknown\", Crema + wav))\n",
    "\n",
    "\n",
    "Crema_df = pd.DataFrame.from_dict(emotion_df)\n",
    "Crema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n",
    "\n",
    "Crema_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TESS dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  Emotion                                               Path\n0     sad  /home/kenterbery/projects/speech_emotion_recog...\n1     sad  /home/kenterbery/projects/speech_emotion_recog...\n2     sad  /home/kenterbery/projects/speech_emotion_recog...\n3     sad  /home/kenterbery/projects/speech_emotion_recog...\n4     sad  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "emotion_df = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    for wav in os.listdir(Tess + dir):\n",
    "        info = wav.partition(\".wav\")[0].split(\"_\")\n",
    "        emo = info[2]\n",
    "        if emo == \"ps\":\n",
    "            emotion_df.append((\"surprise\", Tess + dir + \"/\" + wav))\n",
    "        else:\n",
    "            emotion_df.append((emo, Tess + dir + \"/\" + wav))\n",
    "\n",
    "\n",
    "Tess_df = pd.DataFrame.from_dict(emotion_df)\n",
    "Tess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n",
    "\n",
    "Tess_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Path\n0      sad  /home/kenterbery/projects/speech_emotion_recog...\n1  neutral  /home/kenterbery/projects/speech_emotion_recog...\n2  disgust  /home/kenterbery/projects/speech_emotion_recog...\n3    angry  /home/kenterbery/projects/speech_emotion_recog...\n4  disgust  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sad</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>angry</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savee_directiory_list = os.listdir(Savee)\n",
    "\n",
    "emotion_df = []\n",
    "\n",
    "for wav in savee_directiory_list:\n",
    "    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n",
    "    emotion = re.split(r\"[0-9]\", info)[0]\n",
    "    if emotion=='a':\n",
    "        emotion_df.append((\"angry\", Savee + wav))\n",
    "    elif emotion=='d':\n",
    "        emotion_df.append((\"disgust\", Savee + wav))\n",
    "    elif emotion=='f':\n",
    "        emotion_df.append((\"fear\", Savee + wav))\n",
    "    elif emotion=='h':\n",
    "        emotion_df.append((\"happy\", Savee + wav))\n",
    "    elif emotion=='n':\n",
    "        emotion_df.append((\"neutral\", Savee + wav))\n",
    "    elif emotion=='sa':\n",
    "        emotion_df.append((\"sad\", Savee + wav))\n",
    "    else:\n",
    "        emotion_df.append((\"surprise\", Savee + wav))\n",
    "\n",
    "\n",
    "Savee_df = pd.DataFrame.from_dict(emotion_df)\n",
    "Savee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n",
    "\n",
    "Savee_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(12162, 2)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Path\n0  neutral  /home/kenterbery/projects/speech_emotion_recog...\n1  neutral  /home/kenterbery/projects/speech_emotion_recog...\n2  disgust  /home/kenterbery/projects/speech_emotion_recog...\n3  disgust  /home/kenterbery/projects/speech_emotion_recog...\n4  disgust  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Count of emotions:\")\n",
    "sns.countplot(x=df[\"Emotion\"])\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   Emotion                                               Path\n0  neutral  /home/kenterbery/projects/speech_emotion_recog...\n1  neutral  /home/kenterbery/projects/speech_emotion_recog...\n2  disgust  /home/kenterbery/projects/speech_emotion_recog...\n3  disgust  /home/kenterbery/projects/speech_emotion_recog...\n4  disgust  /home/kenterbery/projects/speech_emotion_recog...\n5    angry  /home/kenterbery/projects/speech_emotion_recog...\n6    angry  /home/kenterbery/projects/speech_emotion_recog...\n7    happy  /home/kenterbery/projects/speech_emotion_recog...\n8     fear  /home/kenterbery/projects/speech_emotion_recog...\n9    angry  /home/kenterbery/projects/speech_emotion_recog...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>angry</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>angry</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>happy</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fear</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>angry</td>\n      <td>/home/kenterbery/projects/speech_emotion_recog...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def create_spectrogram_plot(data, output_path):\n",
    "    # X = librosa.stft(data)\n",
    "    # Xdb = librosa.amplitude_to_db(abs(X), ref=np.max)\n",
    "\n",
    "    # For plotting headlessly\n",
    "    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "    # fig = plt.Figure()\n",
    "    # canvas = FigureCanvas(fig)\n",
    "    # ax = fig.add_subplot(111)\n",
    "    # librosa.display.specshow(Xdb, cmap=\"gray_r\")\n",
    "    # fig.savefig(output_path)\n",
    "\n",
    "    fig = plt.Figure()\n",
    "    # canvas = FigureCanvas(fig)\n",
    "    ax = fig.add_subplot(111)\n",
    "    p = librosa.display.specshow(Xdb, ax=ax, cmap=\"magma\",)\n",
    "    fig.savefig(output_path,  bbox_inches='tight', pad_inches=0)\n",
    "#\n",
    "#\n",
    "def create_spectrogram(data, sr):\n",
    "    X = librosa.stft(data)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X), ref=np.max)\n",
    "\n",
    "    return Xdb\n",
    "\n",
    "def get_mel_spectrogram(path):\n",
    "    data, sr = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    melspec = librosa.feature.melspectrogram(data, sr=sr, n_mels=128)\n",
    "    melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "    return melspec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def fill_to_max_shape(mel):\n",
    "    t = np.pad(mel, ((0,0),(0, max_shape - mel.shape[1])), mode='constant', constant_values=-99)\n",
    "    return t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12162/12162 [15:51<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(12162,)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_df = []\n",
    "\n",
    "print(\"Start processing...\")\n",
    "\n",
    "a = df.Path.progress_apply(get_mel_spectrogram)\n",
    "print(\"Done.\")\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12162/12162 [00:00<00:00, 414253.09it/s]\n"
     ]
    }
   ],
   "source": [
    "max_shape = a.progress_apply(lambda x: x.shape[1]).max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12162/12162 [00:01<00:00, 8517.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(128, 108)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.progress_apply(fill_to_max_shape)\n",
    "a.iloc[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-66.87003 , -56.756348, -55.05352 , ..., -57.27312 , -49.51666 ,\n        -45.69447 ],\n       [-72.44478 , -59.4984  , -58.47601 , ..., -66.96373 , -66.16691 ,\n        -60.22343 ],\n       [-74.28613 , -60.766544, -57.56222 , ..., -65.23785 , -67.93416 ,\n        -65.501396],\n       ...,\n       [-80.67789 , -80.67789 , -80.67789 , ..., -80.67789 , -80.67789 ,\n        -80.67789 ],\n       [-80.67789 , -80.67789 , -80.67789 , ..., -80.67789 , -80.67789 ,\n        -80.67789 ],\n       [-80.67789 , -80.67789 , -80.67789 , ..., -80.67789 , -80.67789 ,\n        -80.67789 ]], dtype=float32)"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MelSpec\"] = a\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(128, 108)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MelSpec\"].iloc[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try to modeling using MelSpec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12162 into shape (12162,128,108,1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-146-a0ee20c64bd4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMelSpec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m108\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mY\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEmotion\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: cannot reshape array of size 12162 into shape (12162,128,108,1)"
     ]
    }
   ],
   "source": [
    "X = df.MelSpec.values.reshape(df.shape[0], 128, 108, 1)\n",
    "Y = df.Emotion.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape\n",
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9729,)\n",
      "(2433,)\n",
      "(9729,)\n",
      "(2433,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.MelSpec.values.reshape, df.Emotion.values, shuffle=True, test_size=.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7783,)\n",
      "(1946,)\n",
      "(2433,)\n",
      "(7783,)\n",
      "(1946,)\n",
      "(2433,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=True, test_size=.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to standartize\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"try to standartize\")\n",
    "scaler = MinMaxScaler()\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = scaler.fit_transform(X_train[i])\n",
    "    X_train[i] = X_train[i].reshape((128, 108, 1))\n",
    "for i in range(X_val.shape[0]):\n",
    "    X_val[i] = scaler.transform(X_val[i])\n",
    "    X_train[i] = X_train[i].reshape((128, 108, 1))\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = scaler.transform(X_test[i])\n",
    "    X_train[i] = X_train[i].reshape((128, 108, 1))\n",
    "print(\"Done\")\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;31mTypeError\u001B[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-143-03f419308d92>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mX_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"float32\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_val = enc.transform(y_val)\n",
    "y_test = enc.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "EPOCHS = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(5,5), padding=\"same\", input_shape=(128, 108, 1), activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, kernel_size=(5,5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dense(7, activation=\"softmax\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=\"acc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 128, 108, 32)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 108, 32)      25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 54, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 13312)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1704064   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,860,711\n",
      "Trainable params: 1,860,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_acc\", patience=8, min_delta=0.0001, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_acc\", patience=5, factor=0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-138-b23ca6152e42>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n\u001B[0;32m----> 2\u001B[0;31m                  batch_size=batch_size, epochs=EPOCHS, callbacks=[early_stopping, reduce_lr])\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    106\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[0;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1061\u001B[0m           \u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1062\u001B[0m           \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1063\u001B[0;31m           steps_per_execution=self._steps_per_execution)\n\u001B[0m\u001B[1;32m   1064\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1065\u001B[0m       \u001B[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001B[0m\n\u001B[1;32m   1115\u001B[0m         \u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1116\u001B[0m         \u001B[0mdistribution_strategy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mds_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1117\u001B[0;31m         model=model)\n\u001B[0m\u001B[1;32m   1118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1119\u001B[0m     \u001B[0mstrategy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mds_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m                **kwargs):\n\u001B[1;32m    264\u001B[0m     \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTensorLikeDataAdapter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m     \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_tensorlike\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001B[1;32m    267\u001B[0m         sample_weights, sample_weight_modes)\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m_process_tensorlike\u001B[0;34m(inputs)\u001B[0m\n\u001B[1;32m   1019\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1021\u001B[0;31m   \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_convert_numpy_and_scipy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1022\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist_to_tuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    633\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 635\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    637\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    633\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 635\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    637\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36m_convert_numpy_and_scipy\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m   1014\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloating\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1015\u001B[0m         \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloatx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1016\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1017\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mscipy_sparse\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mscipy_sparse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1018\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_scipy_sparse_to_sparse_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[1;32m   1497\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1498\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1499\u001B[0;31m       \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconversion_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mas_ref\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1500\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1501\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001B[0m in \u001B[0;36m_default_conversion_function\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_default_conversion_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m   \u001B[0;32mdel\u001B[0m \u001B[0mas_ref\u001B[0m  \u001B[0;31m# Unused.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mconstant_op\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstant\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    262\u001B[0m   \"\"\"\n\u001B[1;32m    263\u001B[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001B[0;32m--> 264\u001B[0;31m                         allow_broadcast=True)\n\u001B[0m\u001B[1;32m    265\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    273\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tf.constant\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 275\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    277\u001B[0m   \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36m_constant_eager_impl\u001B[0;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[1;32m    298\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    299\u001B[0m   \u001B[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 300\u001B[0;31m   \u001B[0mt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_to_eager_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    301\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/speech_emotion_recognition/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m     96\u001B[0m       \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m   \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEagerTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                 batch_size=batch_size, epochs=EPOCHS, callbacks=[early_stopping, reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}